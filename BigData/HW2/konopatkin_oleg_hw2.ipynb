{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 1 ноября 2018, 06:00 \n",
    "**Штраф за опоздание:** -2 балла после 06:00 1 ноября, -4 балла после 06:00 8 ноября, -6 баллов после 06:00 15 ноября\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "Необходимо в slack создать таск в приватный чат:   \n",
    "/todo Фамилия Имя *ссылка на гитхаб* @alkhamush   \n",
    "Пример:   \n",
    "/todo Ксения Стройкова https://github.com/stroykova/spheremailru/stroykova_hw2.ipynb @alkhamush   \n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Задание 1 (4 балла)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. В комментариях, где написано \"Что делает этот блок кода?\", ответьте на этот вопрос. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (2 балла)\n",
    "Добиться скорости работы fit такой, чтобы она была медленнее sklearn не более чем в 10 раз. Скорость проверяем на  wine и Speed Dating Data. Для ускорения используем только numpy.\n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 3 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "\n",
    "class MyDecisionTreeClassifier():\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=None,\n",
    "                 sufficient_share=1.0, criterion='gini', max_features=None):\n",
    "        self.params = {'min_samples_split': min_samples_split,\n",
    "                       'max_depth': max_depth,\n",
    "                       'criterion': criterion,\n",
    "                       'sufficient_share': sufficient_share,\n",
    "                       'max_features': max_features}\n",
    "\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features is None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s_1, r_s_1 = l_s.reshape(l_s.shape[0]), r_s.reshape(r_s.shape[0])\n",
    "        l_part, r_part = (l_s_1 / (l_s_1 + r_s_1)), (r_s_1 / (l_s_1 + r_s_1))\n",
    "        return ((np.ones(l_s.shape[1]) - np.sum(np.square(l_c / l_s), axis=-1))\n",
    "                * l_part +\n",
    "                (np.ones(r_s.shape[1]) - np.sum(np.square(r_c / r_s), axis=-1))\n",
    "                * r_part)\n",
    "\n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        l_s_1, r_s_1 = l_s.reshape(l_s.shape[0]), r_s.reshape(r_s.shape[0])\n",
    "        l_part, r_part = (l_s_1 / (l_s_1 + r_s_1)), (r_s_1 / (l_s_1 + r_s_1))\n",
    "        p_l, p_r = l_c / l_s, r_c / r_s\n",
    "        # несмотря на where кидает предупреждение о log(0)\n",
    "        return ((-np.sum(np.where(p_l == 0, p_l, p_l * np.log(p_l)))) * l_part\n",
    "                +\n",
    "                (-np.sum(np.where(p_r == 0, p_r, p_r * np.log(p_r)))) * r_part)\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        l_s_1, r_s_1 = l_s.reshape(l_s.shape[0]), r_s.reshape(r_s.shape[0])\n",
    "        l_part, r_part = (l_s_1 / (l_s_1 + r_s_1)), (r_s_1 / (l_s_1 + r_s_1))\n",
    "        p_l = np.append(l_c / l_s, np.zeros(l_s.shape), axis=-1)\n",
    "        p_r = np.append(r_c / r_s, np.zeros(r_s.shape), axis=-1)\n",
    "        return ((np.ones(l_s.shape[1]) - np.max(p_l, axis=-1)) * l_part +\n",
    "                (np.ones(r_s.shape[1]) - np.max(p_r, axis=-1)) * r_part)\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = np.array(range(n_feature))\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:max(1, int(np.ceil(np.sqrt(n_feature))))]\n",
    "\n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = np.array(range(n_feature))\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return feature_ids[:max(1, int(np.ceil(np.log2(n_feature))))]\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return np.array(range(n_feature))\n",
    "\n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "        # Что делает этот блок кода?\n",
    "        # - сортирует объекты по фиче x, считает минимальное число объектов\n",
    "        #   в одной из вершин разбиения\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)\n",
    "        class_number = self.num_class\n",
    "        cut_size = np.int(self.min_samples_split / 2 - 1)\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # - находит отрезки из одинаковых объектов, сохраняет их границы;\n",
    "        #   если в центре (без хвостов) все объекты одинаковые,\n",
    "        #   то разделить нельзя, возвращаем impurity = inf\n",
    "        if cut_size == 0:\n",
    "            splitted_sorted_y = sorted_y\n",
    "        else:\n",
    "            splitted_sorted_y = sorted_y[cut_size:-cut_size]\n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1] !=\n",
    "                                splitted_sorted_y[1:])[0] + (cut_size + 1)\n",
    "\n",
    "        if len(r_border_ids) == 0:\n",
    "            return np.inf, None\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # - считает для каждого разбиения по одному из объектов из r_border_ids\n",
    "        #   сколько объектов какого класса попадут левее данного объекта\n",
    "        eq_el_count = r_border_ids - np.append(np.array([cut_size]),\n",
    "                                               r_border_ids[:-1])\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], class_number))\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]),\n",
    "                     sorted_y[r_border_ids - 1]] = 1\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        class_increments[0] = class_increments[0] + \\\n",
    "            np.bincount(sorted_y[:cut_size], minlength=class_number)\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # - считает для каждого разбиения по одному из border_ids сколько\n",
    "        #   объектов какого класса попадут в левую и правую вершины, а также\n",
    "        #   общее количество объектов в разбиениях\n",
    "        l_class_count = np.cumsum(class_increments, axis=0)\n",
    "        r_class_count = np.bincount(sorted_y,\n",
    "                                    minlength=class_number) - l_class_count\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # - вычисляет impurity каждого разбиения,\n",
    "        #   находит оптимальное с min impurity\n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        #####\n",
    "        # проверка на совпадающие иксы и невозможность разделить\n",
    "        # без неё почему-то работает в 50 раз быстрее, но точность\n",
    "        # на SpeedDating становится меньшь 50%...\n",
    "        ####\n",
    "        gs = gs[sorted_x[l_sizes[:, 0] - 1] != sorted_x[l_sizes[:, 0]]]\n",
    "        if gs.shape[0] == 0:\n",
    "            return np.inf, None\n",
    "        #####\n",
    "        # работает быстрее, потому что на первом же разделении натыкается на\n",
    "        # разбиение, где в одном листе пусто, и корень сразу объявляется листом...\n",
    "        #####\n",
    "        idx = np.argmin(gs)\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # - возвращает impurity и границу разбиения\n",
    "        #   для полученного оптимального разбиения\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        border = (sorted_x[left_el_id-1] + sorted_x[left_el_id]) / 2.0\n",
    "        return gs[idx], (sorted_x[left_el_id-1] + sorted_x[left_el_id]) / 2.0\n",
    "\n",
    "    def __make_leaf(self, node_id, features, shares):\n",
    "        feature_id = features[0][np.argmax(shares)]\n",
    "        node = (self.LEAF_TYPE, feature_id, 0.0)\n",
    "        self.tree[node_id] = node\n",
    "        return\n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth, pred_f=-1):\n",
    "        # Ваш код\n",
    "        # Необходимо использовать следующее:\n",
    "        # self.LEAF_TYPE     _, feature_id, threshold = node\n",
    "        # self.NON_LEAF_TYPE\n",
    "\n",
    "        # self.tree\n",
    "        # self.max_depth\n",
    "        # self.sufficient_share\n",
    "        # self.min_samples_split\n",
    "\n",
    "        # self.get_feature_ids\n",
    "        # self.__find_threshold\n",
    "        # self.__div_samples\n",
    "        # self.__fit_node\n",
    "\n",
    "        features = np.unique(y, return_counts=True)\n",
    "        shares = features[1] / np.sum(features[1])\n",
    "        if ((self.max_depth is not None and depth >= self.max_depth) or\n",
    "                (np.max(shares) >= self.sufficient_share) or\n",
    "                (y.shape[0] <= self.min_samples_split)):\n",
    "            self.__make_leaf(node_id, features, shares)\n",
    "            return\n",
    "\n",
    "        feature_ids = self.get_feature_ids(x.shape[1])\n",
    "\n",
    "        splits = [(*self.__find_threshold(x[:, f_id], y), f_id)\n",
    "                  for f_id in feature_ids if f_id != pred_f]\n",
    "        # нужно сравнивать кортежи целиком\n",
    "        gs, threshold, feature_id = min(splits)\n",
    "\n",
    "        if (threshold is None):\n",
    "            self.__make_leaf(node_id, features, shares)\n",
    "            return\n",
    "\n",
    "        pred_gs = self.G_function(\n",
    "            np.array([np.unique(y, return_counts=True)[1]]),\n",
    "            np.array([[y.shape[0]]]),\n",
    "            np.array([[]]), np.array([[0]]))\n",
    "\n",
    "        self.feature_importances_[feature_id] += (pred_gs - gs)\n",
    "\n",
    "        node = (self.NON_LEAF_TYPE, feature_id, threshold)\n",
    "        self.tree[node_id] = node\n",
    "\n",
    "        x_l, x_r, y_l, y_r = self.__div_samples(x, y, feature_id, threshold)\n",
    "\n",
    "        if (y_l.shape[0] == 0 or y_r.shape[0] == 0):\n",
    "            self.__make_leaf(node_id, features, shares)\n",
    "            return\n",
    "\n",
    "        self.__fit_node(x_l, y_l, node_id * 2 + 1, depth + 1, feature_id)\n",
    "        self.__fit_node(x_r, y_r, node_id * 2 + 2, depth + 1, feature_id)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.feature_importances_ = np.zeros(x.shape[1])\n",
    "        self.__fit_node(x, y, 0, 0)\n",
    "        self.feature_importances_ /= np.sum(self.feature_importances_)\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        return f1_score(y_pred=self.predict(X_test),\n",
    "                        y_true=y_test, average='macro')\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.__init__(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1e+03 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# тут делаете то же самое, что и на семинаре https://github.com/stroykova/spheremailru/blob/master/2018-02/lecture_04_trees/pract-speed-dating-trees-proc.ipynb\n",
    "df = pd.read_csv('speed-dating-experiment/Speed Dating Data.csv', \n",
    "                 encoding='cp1251')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.iloc[:, :97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['id'], axis=1)\n",
    "df = df.drop(['idg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['condtn'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['round'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['position', 'positin1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['order'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['partner'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['age_o', 'race_o', 'pf_o_att', \n",
    "              'pf_o_sin', 'pf_o_int',\n",
    "              'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "              'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "              'amb_o', 'shar_o', 'like_o', 'prob_o','met_o'], \n",
    "             axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['field'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['undergra'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Что будем делать?\n",
    "df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Что будем делать?\n",
    "df.loc[:, 'tuition'] = df.tuition.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Что делать?\n",
    "\n",
    "df = df.dropna(subset=['imprelig', 'imprace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['from', 'zipcode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.drop(['income'], axis=1)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['career'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['sports','tvsports','exercise','dining','museums','art','hiking','gaming',\n",
    "       'clubbing','reading','tv','theater','movies','concerts','music','shopping','yoga'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['expnum'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat = ['iid', 'wave', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = ((temp.wave >= 6) & (temp.wave <= 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "(df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].T/df.loc[:, 'temp_totalsum'].T).T * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat = ['iid', 'wave', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 90) & (temp.totalsum != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = ((temp.wave >= 6) & (temp.wave <= 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "(df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']].T/df.loc[:, 'temp_totalsum'].T).T * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['temp_totalsum'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [4, 5]:\n",
    "    feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i), \n",
    "            'intel{}_1'.format(i), 'fun{}_1'.format(i), \n",
    "            'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "    \n",
    "    if i != 4:\n",
    "        feat.remove('shar{}_1'.format(i))\n",
    "    \n",
    "    df = df.drop(feat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['wave'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid'])\\\n",
    "                                 .drop(['gender'], axis=1)\\\n",
    "                                 .dropna()\n",
    "df_female = df.query('gender == 0').drop_duplicates(subset=['iid'])\\\n",
    "                                   .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1)\\\n",
    "                                   .dropna()\n",
    "        \n",
    "df_female.columns = df_female.columns + '_f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_female = df_female.drop(['pid_f'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pair = df_male.join(df_female.set_index('iid_f'), \n",
    "                       on='pid', \n",
    "                       how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pair = df_pair.drop(['iid', 'pid'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th>int_corr</th>\n",
       "      <th>samerace</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>mn_sat</th>\n",
       "      <th>tuition</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc2_1_f</th>\n",
       "      <th>intel2_1_f</th>\n",
       "      <th>fun2_1_f</th>\n",
       "      <th>amb2_1_f</th>\n",
       "      <th>shar2_1_f</th>\n",
       "      <th>attr3_1_f</th>\n",
       "      <th>sinc3_1_f</th>\n",
       "      <th>fun3_1_f</th>\n",
       "      <th>intel3_1_f</th>\n",
       "      <th>amb3_1_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3489</th>\n",
       "      <td>0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1450.0</td>\n",
       "      <td>26100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>13258.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>26908.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3516</th>\n",
       "      <td>0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1140.0</td>\n",
       "      <td>9168.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>26062.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      match  int_corr  samerace   age  field_cd  mn_sat  tuition  race  \\\n",
       "3489      0      0.43         0  27.0      16.0  1450.0  26100.0   2.0   \n",
       "3498      0      0.46         0  28.0       8.0  1155.0  13258.0   2.0   \n",
       "3507      0      0.05         0  32.0       7.0  1430.0  26908.0   2.0   \n",
       "3516      0      0.44         0  25.0      10.0  1140.0   9168.0   2.0   \n",
       "3534      0      0.23         1  27.0       8.0  1360.0  26062.0   6.0   \n",
       "\n",
       "      imprace  imprelig    ...     sinc2_1_f  intel2_1_f  fun2_1_f  amb2_1_f  \\\n",
       "3489      2.0       3.0    ...           1.0         1.0       3.0      10.0   \n",
       "3498      8.0       9.0    ...           1.0         1.0       3.0      10.0   \n",
       "3507      1.0       6.0    ...           1.0         1.0       3.0      10.0   \n",
       "3516      7.0       1.0    ...           1.0         1.0       3.0      10.0   \n",
       "3534      1.0       1.0    ...           1.0         1.0       3.0      10.0   \n",
       "\n",
       "      shar2_1_f  attr3_1_f  sinc3_1_f  fun3_1_f  intel3_1_f  amb3_1_f  \n",
       "3489       10.0        7.0        7.0       8.0         8.0      10.0  \n",
       "3498       10.0        7.0        7.0       8.0         8.0      10.0  \n",
       "3507       10.0        7.0        7.0       8.0         8.0      10.0  \n",
       "3516       10.0        7.0        7.0       8.0         8.0      10.0  \n",
       "3534       10.0        7.0        7.0       8.0         8.0      10.0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values\n",
    "df_pair.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа %time clf.fit(X_train, y_train)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# параметры из GridSearchCV\n",
    "my_clf.set_params(**{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 260 ms\n"
     ]
    }
   ],
   "source": [
    "# тут должен быть код типа %time my_clf.fit(X_train, y_train)\n",
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57328385899814471"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')\n",
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60949988581868009"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут должен быть код типа f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')\n",
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best clf features:\n",
      "int_corr : 0.0687644546359\n",
      "field_cd : 0.0544632098921\n",
      "amb3_1_f : 0.0488464590217\n",
      "fun2_1 : 0.0396190460126\n",
      "imprelig : 0.038623118189\n",
      "shar2_1 : 0.0380803436396\n",
      "shar1_1 : 0.0350521039305\n",
      "amb1_1_f : 0.0338577648424\n",
      "tuition_f : 0.0304488457531\n",
      "amb1_1 : 0.0303318856964\n",
      "\n",
      "Best my_clf features:\n",
      "int_corr : 0.109703966453\n",
      "field_cd_f : 0.0615813368467\n",
      "fun3_1 : 0.0548815154768\n",
      "shar1_1 : 0.0478295449812\n",
      "imprace_f : 0.0473976724572\n",
      "date_f : 0.0470096213377\n",
      "amb1_1 : 0.0430649212354\n",
      "field_cd : 0.0419095209096\n",
      "amb2_1_f : 0.0368435348656\n",
      "career_c_f : 0.0342593702673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_clf = sorted(zip(df_pair.keys()[1:], clf.feature_importances_), key = lambda x: -x[1])\n",
    "print(\"Best clf features:\")\n",
    "for feature in best_clf[:10]:\n",
    "    print(feature[0], ':', feature[1])\n",
    "print()\n",
    "\n",
    "best_my_clf = sorted(zip(df_pair.keys()[1:], my_clf.feature_importances_), key = lambda x: -x[1])\n",
    "print(\"Best my_clf features:\")\n",
    "for feature in best_my_clf[:10]:\n",
    "    print(feature[0], ':', feature[1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 3, 'max_features': 'log2', 'min_samples_split': 2}\n",
      "0.857782754759\n"
     ]
    }
   ],
   "source": [
    "params = {'min_samples_split': list(range(2, 5)), \n",
    "          'max_depth': [None] + list(range(3, 6)),\n",
    "          'criterion': ['gini', 'entropy'], \n",
    "          'max_features': [None, 'sqrt', 'log2']}\n",
    "optimiser = GridSearchCV(DecisionTreeClassifier(), params)\n",
    "optimiser.fit(X, y)\n",
    "print(optimiser.best_params_)\n",
    "print(optimiser.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 4}\n",
      "0.521316389781\n"
     ]
    }
   ],
   "source": [
    "params = {'min_samples_split': list(range(2, 5)), \n",
    "          'max_depth': [None] + list(range(3, 6)),\n",
    "          'criterion': ['gini', 'entropy', 'misclass'], \n",
    "          'max_features': [None, 'sqrt', 'log2']}\n",
    "# optimiser = GridSearchCV(MyDecisionTreeClassifier(), params,refit=False)\n",
    "optimiser = GridSearchCV(MyDecisionTreeClassifier(), params,refit=False)\n",
    "optimiser.fit(X, y)\n",
    "print(optimiser.best_params_)\n",
    "print(optimiser.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# если закомментировать кусок кода, выкидывающий часть gs, которые невозможно получить, \n",
    "# то максимальная точность получается меньше 50%, что совсем плохо для бинарного классификатора,\n",
    "# но fit выполняется около 16ms против 13ms у sklearn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
